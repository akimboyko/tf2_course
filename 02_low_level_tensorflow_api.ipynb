{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Level TensorFlow API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to use TensorFlow's low-level API, then use it to build custom loss functions, as well as custom Keras layers and models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "matplotlib 3.0.2\n",
      "numpy 1.15.4\n",
      "pandas 0.23.4\n",
      "sklearn 0.20.1\n",
      "tensorflow 2.0.0-dev20190126\n",
      "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "print(\"python\", sys.version)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can browse through the code examples or jump directly to the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=10, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=13, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=15, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=19, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To/From NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=22, shape=(2, 3), dtype=float64, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1., 2., 3.], [4., 5., 6.]])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=26, shape=(), dtype=float32, numpy=2.718>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant(2.718)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conflicting Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute Add as input #0(zero-based) was expected to be a float tensor but is a int32 tensor [Op:Add] name: add/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(1) + tf.constant(1.0)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute Add as input #0(zero-based) was expected to be a float tensor but is a double tensor [Op:Add] name: add/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(1.0, dtype=tf.float64) + tf.constant(1.0)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=36, shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant(1.0, dtype=tf.float64)\n",
    "tf.cast(t, tf.float32) + tf.constant(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=38, shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant(\"café\")\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=40, shape=(), dtype=int32, numpy=5>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.length(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=42, shape=(), dtype=int32, numpy=4>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.length(t, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=47, shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233], dtype=int32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_decode(t, \"UTF8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.constant([\"Café\", \"Coffee\", \"caffè\", \"咖啡\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=50, shape=(4,), dtype=int32, numpy=array([4, 6, 5, 2], dtype=int32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.length(t, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.RaggedTensor(values=tf.Tensor(\n",
       "[   67    97   102   233    67   111   102   102   101   101    99    97\n",
       "   102   102   232 21654 21857], shape=(17,), dtype=int32), row_splits=tf.Tensor([ 0  4 10 15 17], shape=(5,), dtype=int64))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = tf.strings.unicode_decode(t, \"UTF8\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragged tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.RaggedTensor(values=tf.Tensor([11 12 21 22 23 41], shape=(6,), dtype=int32), row_splits=tf.Tensor([0 2 5 5 6], shape=(5,), dtype=int64))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = tf.ragged.constant([[11, 12], [21, 22, 23], [], [41]])\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[11, 12], [21, 22, 23], [], [41]]>\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([21 22 23], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[21, 22, 23]]>\n"
     ]
    }
   ],
   "source": [
    "print(r[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[11, 12], [21, 22, 23], [], [41], [51, 52], [], [71]]>\n"
     ]
    }
   ],
   "source": [
    "r2 = tf.ragged.constant([[51, 52], [], [71]])\n",
    "print(tf.concat([r, r2], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[11, 12, 13, 14, 15], [21, 22, 23, 24], [], [41, 42, 43]]>\n"
     ]
    }
   ],
   "source": [
    "r3 = tf.ragged.constant([[13, 14, 15], [24], [], [42, 43]])\n",
    "print(tf.concat([r, r3], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=281, shape=(4, 3), dtype=int32, numpy=\n",
       "array([[11, 12,  0],\n",
       "       [21, 22, 23],\n",
       "       [ 0,  0,  0],\n",
       "       [41,  0,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [2 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "s = tf.SparseTensor(indices=[[0, 1], [1, 0], [2, 3]],\n",
    "                    values=[1., 2., 3.],\n",
    "                    dense_shape=[3, 4])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=290, shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [2., 0., 0., 0.],\n",
       "       [0., 0., 0., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s * 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupported operand type(s) for +: 'SparseTensor' and 'float'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    s3 = s + 1.\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=295, shape=(3, 2), dtype=float32, numpy=\n",
       "array([[ 30.,  40.],\n",
       "       [ 20.,  40.],\n",
       "       [210., 240.]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s4 = tf.constant([[10., 20.], [30., 40.], [50., 60.], [70., 80.]])\n",
    "tf.sparse.sparse_dense_matmul(s, s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 2]\n",
      " [0 1]], shape=(2, 2), dtype=int64), values=tf.Tensor([1. 2.], shape=(2,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "s5 = tf.SparseTensor(indices=[[0, 2], [0, 1]],\n",
    "                     values=[1., 2.],\n",
    "                     dense_shape=[3, 4])\n",
    "print(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices[1] = [0,1] is out of order [Op:SparseToDense]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.sparse.to_dense(s5)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=310, shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 2., 1., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s6 = tf.sparse.reorder(s5)\n",
    "tf.sparse.to_dense(s6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=322, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 7.,  8.,  9.]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[1].assign([7., 8., 9.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ResourceVariable' object does not support item assignment\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    v[1] = [7., 8., 9.]\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[4., 5., 6.],\n",
       "       [1., 2., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_delta = tf.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]],\n",
    "                                indices=[1, 0])\n",
    "v.scatter_update(sparse_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,   5.,   6.],\n",
       "       [  1.,   2., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]],\n",
    "                    updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    t = tf.constant([[1., 2., 3.], [4., 5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        t2 = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "    print(t2.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 – Custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading and preparing the California housing dataset. We first load it, then split it into a training set, a validation set and a test set, and finally we scale it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Create an `my_mse()` function with two arguments: the true labels `y_true` and the model predictions `y_pred`. Make it return the mean squared error using TensorFlow operations. Note that you could write your own custom metrics in exactly the same way. **Tip**: recall that the MSE is the mean of the squares of prediction errors, which are the differences between the predictions and the labels, so you will need to use `tf.reduce_mean()` and `tf.square()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "False\n",
      "tf.Tensor(True, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "#assert my_mse(tf.constant([[1., 2., 3.], [4., 5., 6.]]), tf.constant([[1., 2., 3.], [4., 5., 6.]])) == 0.0\n",
    "\n",
    "print(my_mse(tf.constant([[1., 2., 3.], [4., 5., 6.]]), tf.constant([[1., 2., 3.], [4., 5., 6.]])))\n",
    "print(tf.constant(0.0))\n",
    "print(my_mse(tf.constant([[1., 2., 3.], [4., 5., 6.]]), tf.constant([[1., 2., 3.], [4., 5., 6.]])) == \n",
    "      tf.constant(0.0))\n",
    "print(tf.equal(x=my_mse(tf.constant([[1., 2., 3.], [4., 5., 6.]]), tf.constant([[1., 2., 3.], [4., 5., 6.]])),\n",
    "        y=tf.constant(0.0)))\n",
    "\n",
    "tf.debugging.assert_equal(\n",
    "    x=my_mse(tf.constant([[1., 2., 3.], [4., 5., 6.]]), tf.constant([[1., 2., 3.], [4., 5., 6.]])),\n",
    "    y=tf.constant(0.0))\n",
    "\n",
    "tf.debugging.assert_equal(\n",
    "    x=my_mse(tf.constant([[1., 2., 3.], [4., 5., 6.]]), tf.constant([[1., 2., 3.5], [4., 5., 5.5]])),\n",
    "    y=tf.constant(0.083333336))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Compile the following model, passing it your custom loss function, then train it and evaluate it. **Tip**: don't forget to use the scaled sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    return keras.models.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", input_shape=X_train_scaled.shape[1:]),\n",
    "        keras.layers.Dense(1),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_mse, optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610==============================] - 0s 33us/sample - loss: 2.0748 - acc: 0.0023 - val_loss: 0.8141 - val_acc: 0.0044\n",
      "Epoch 2/100\n",
      "11610/11610==============================] - 0s 27us/sample - loss: 0.7675 - acc: 0.0029 - val_loss: 0.7013 - val_acc: 0.0044\n",
      "Epoch 3/100\n",
      "11610/11610==============================] - 0s 27us/sample - loss: 0.7056 - acc: 0.0029 - val_loss: 0.6716 - val_acc: 0.0044\n",
      "Epoch 4/100\n",
      "11610/11610==============================] - 0s 28us/sample - loss: 0.6694 - acc: 0.0029 - val_loss: 0.6443 - val_acc: 0.0044\n",
      "Epoch 5/100\n",
      "11610/11610==============================] - 0s 30us/sample - loss: 0.6390 - acc: 0.0029 - val_loss: 0.6012 - val_acc: 0.0044\n",
      "Epoch 6/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.6109 - acc: 0.0029 - val_loss: 0.6055 - val_acc: 0.0044\n",
      "Epoch 7/100\n",
      "11610/11610==============================] - 0s 33us/sample - loss: 0.5849 - acc: 0.0029 - val_loss: 0.6220 - val_acc: 0.0044\n",
      "Epoch 8/100\n",
      "11610/11610==============================] - 0s 33us/sample - loss: 0.5651 - acc: 0.0029 - val_loss: 0.5816 - val_acc: 0.0044\n",
      "Epoch 9/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.5454 - acc: 0.0029 - val_loss: 0.5325 - val_acc: 0.0044\n",
      "Epoch 10/100\n",
      "11610/11610==============================] - 0s 31us/sample - loss: 0.5284 - acc: 0.0029 - val_loss: 0.5113 - val_acc: 0.0044\n",
      "Epoch 11/100\n",
      "11610/11610==============================] - 0s 31us/sample - loss: 0.5117 - acc: 0.0029 - val_loss: 0.5100 - val_acc: 0.0044\n",
      "Epoch 12/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4988 - acc: 0.0029 - val_loss: 0.4812 - val_acc: 0.0044\n",
      "Epoch 13/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.4853 - acc: 0.0029 - val_loss: 0.4941 - val_acc: 0.0044\n",
      "Epoch 14/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4757 - acc: 0.0029 - val_loss: 0.4593 - val_acc: 0.0044\n",
      "Epoch 15/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4662 - acc: 0.0029 - val_loss: 0.4534 - val_acc: 0.0044\n",
      "Epoch 16/100\n",
      "11610/11610==============================] - 0s 33us/sample - loss: 0.4578 - acc: 0.0029 - val_loss: 0.4308 - val_acc: 0.0044\n",
      "Epoch 17/100\n",
      "11610/11610==============================] - 0s 32us/sample - loss: 0.4513 - acc: 0.0029 - val_loss: 0.4240 - val_acc: 0.0044\n",
      "Epoch 18/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4454 - acc: 0.0028 - val_loss: 0.4229 - val_acc: 0.0044\n",
      "Epoch 19/100\n",
      "11610/11610==============================] - 0s 32us/sample - loss: 0.4403 - acc: 0.0028 - val_loss: 0.4270 - val_acc: 0.0044\n",
      "Epoch 20/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4357 - acc: 0.0028 - val_loss: 0.4106 - val_acc: 0.0044\n",
      "Epoch 21/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4319 - acc: 0.0028 - val_loss: 0.4065 - val_acc: 0.0044\n",
      "Epoch 22/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4286 - acc: 0.0028 - val_loss: 0.4039 - val_acc: 0.0044\n",
      "Epoch 23/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4254 - acc: 0.0028 - val_loss: 0.4007 - val_acc: 0.0044\n",
      "Epoch 24/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4228 - acc: 0.0028 - val_loss: 0.4185 - val_acc: 0.0044\n",
      "Epoch 25/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4204 - acc: 0.0028 - val_loss: 0.3964 - val_acc: 0.0044\n",
      "Epoch 26/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.4183 - acc: 0.0028 - val_loss: 0.4126 - val_acc: 0.0044\n",
      "Epoch 27/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4161 - acc: 0.0028 - val_loss: 0.3989 - val_acc: 0.0044\n",
      "Epoch 28/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.4143 - acc: 0.0028 - val_loss: 0.4069 - val_acc: 0.0044\n",
      "Epoch 29/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4126 - acc: 0.0028 - val_loss: 0.4054 - val_acc: 0.0044\n",
      "Epoch 30/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4111 - acc: 0.0028 - val_loss: 0.3895 - val_acc: 0.0044\n",
      "Epoch 31/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4095 - acc: 0.0028 - val_loss: 0.3883 - val_acc: 0.0044\n",
      "Epoch 32/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4083 - acc: 0.0028 - val_loss: 0.4170 - val_acc: 0.0044\n",
      "Epoch 33/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4069 - acc: 0.0028 - val_loss: 0.3831 - val_acc: 0.0044\n",
      "Epoch 34/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4057 - acc: 0.0028 - val_loss: 0.4124 - val_acc: 0.0044\n",
      "Epoch 35/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4046 - acc: 0.0028 - val_loss: 0.4058 - val_acc: 0.0044\n",
      "Epoch 36/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4036 - acc: 0.0028 - val_loss: 0.3828 - val_acc: 0.0044\n",
      "Epoch 37/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.4024 - acc: 0.0028 - val_loss: 0.4195 - val_acc: 0.0044\n",
      "Epoch 38/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4017 - acc: 0.0028 - val_loss: 0.3826 - val_acc: 0.0044\n",
      "Epoch 39/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4006 - acc: 0.0028 - val_loss: 0.3814 - val_acc: 0.0044\n",
      "Epoch 40/100\n",
      "11610/11610==============================] - 0s 30us/sample - loss: 0.3997 - acc: 0.0028 - val_loss: 0.3753 - val_acc: 0.0044\n",
      "Epoch 41/100\n",
      "11610/11610==============================] - 0s 23us/sample - loss: 0.3989 - acc: 0.0028 - val_loss: 0.3890 - val_acc: 0.0044\n",
      "Epoch 42/100\n",
      "11610/11610==============================] - 0s 23us/sample - loss: 0.3981 - acc: 0.0028 - val_loss: 0.3821 - val_acc: 0.0044\n",
      "Epoch 43/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.3973 - acc: 0.0028 - val_loss: 0.3941 - val_acc: 0.0044\n",
      "Epoch 44/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.3967 - acc: 0.0028 - val_loss: 0.3784 - val_acc: 0.0044\n",
      "Epoch 45/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.3957 - acc: 0.0028 - val_loss: 0.4276 - val_acc: 0.0044\n",
      "5160/5160==============================] - 0s 13us/sample - loss: 0.3923 - acc: 0.0019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3922614634961121, 0.0019379845]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid),\n",
    "                   callbacks=[\n",
    "                       keras.callbacks.EarlyStopping(patience=5)\n",
    "                   ])\n",
    "\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': [0.0023255814,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772],\n",
      " 'loss': [2.0748008945191554,\n",
      "          0.7674978333336849,\n",
      "          0.7056191052628631,\n",
      "          0.6693815261407929,\n",
      "          0.6390139236622695,\n",
      "          0.6108543507299169,\n",
      "          0.5849213166446341,\n",
      "          0.5651393763214017,\n",
      "          0.5454346868110873,\n",
      "          0.5284094098207768,\n",
      "          0.5117466359380809,\n",
      "          0.4988471601309189,\n",
      "          0.48534778409944745,\n",
      "          0.4756676964262865,\n",
      "          0.466192523279034,\n",
      "          0.4577901574068291,\n",
      "          0.4512839290214755,\n",
      "          0.44538203820291006,\n",
      "          0.44033162101756695,\n",
      "          0.435708589619547,\n",
      "          0.43190139748740053,\n",
      "          0.4286279289862085,\n",
      "          0.42537437811891754,\n",
      "          0.4227930780683069,\n",
      "          0.42036262674643804,\n",
      "          0.4182574605910969,\n",
      "          0.41609421563908316,\n",
      "          0.41434467644859857,\n",
      "          0.41259420174755584,\n",
      "          0.4111306130372687,\n",
      "          0.4095334741907013,\n",
      "          0.4082811420586066,\n",
      "          0.4069478643494983,\n",
      "          0.4057164542742786,\n",
      "          0.4046233630406251,\n",
      "          0.403610870450042,\n",
      "          0.40242975338279535,\n",
      "          0.4016558476145364,\n",
      "          0.4006088445969022,\n",
      "          0.3996841056145644,\n",
      "          0.3989421158746642,\n",
      "          0.3981494841444195,\n",
      "          0.39730770557117706,\n",
      "          0.39668494801804693,\n",
      "          0.3957244423034374],\n",
      " 'val_acc': [0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765],\n",
      " 'val_loss': [0.8140732670323177,\n",
      "              0.7012597605855582,\n",
      "              0.6715556770486117,\n",
      "              0.644348803262686,\n",
      "              0.6011690397287216,\n",
      "              0.605543401506212,\n",
      "              0.6220017317807643,\n",
      "              0.5815681203083166,\n",
      "              0.5325419507285422,\n",
      "              0.5112594677958378,\n",
      "              0.5100194413994634,\n",
      "              0.48116052158427175,\n",
      "              0.49409297493693133,\n",
      "              0.4592870269788944,\n",
      "              0.4533728792993905,\n",
      "              0.4308204597126914,\n",
      "              0.4240482685947911,\n",
      "              0.42287071682994065,\n",
      "              0.4270422308789975,\n",
      "              0.4106095877112652,\n",
      "              0.4065440557723822,\n",
      "              0.4039027628818532,\n",
      "              0.40069182710746154,\n",
      "              0.4185248604424548,\n",
      "              0.39643980571157866,\n",
      "              0.41257227767037485,\n",
      "              0.3989343090112819,\n",
      "              0.4068931742694027,\n",
      "              0.40536495930772726,\n",
      "              0.38947546224877505,\n",
      "              0.38827691269167325,\n",
      "              0.41695870122552225,\n",
      "              0.38308304117015474,\n",
      "              0.41239933015764224,\n",
      "              0.4058402262916861,\n",
      "              0.3827618733703012,\n",
      "              0.4195473810653046,\n",
      "              0.3825906852265045,\n",
      "              0.3814390173870156,\n",
      "              0.3753400472091458,\n",
      "              0.3889798132948173,\n",
      "              0.38214087181313094,\n",
      "              0.3940551082293193,\n",
      "              0.3783973534131851,\n",
      "              0.4275889846397617]}\n"
     ]
    }
   ],
   "source": [
    "pprint(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Try building and compiling the model again, this time adding `\"mse\"` (or equivalently `\"mean_squared_error\"` or `keras.losses.mean_squared_error`) to the list of additional metrics, then train the model and make sure the `my_mse` is equal to the standard `mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610==============================] - 1s 48us/sample - loss: 2.0733 - acc: 0.0023 - val_loss: 1.6607 - val_acc: 0.0044\n",
      "Epoch 2/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.7719 - acc: 0.0029 - val_loss: 0.6980 - val_acc: 0.0044\n",
      "Epoch 3/100\n",
      "11610/11610==============================] - 1s 44us/sample - loss: 0.7070 - acc: 0.0029 - val_loss: 0.6523 - val_acc: 0.0044\n",
      "Epoch 4/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.6688 - acc: 0.0029 - val_loss: 0.6655 - val_acc: 0.0044\n",
      "Epoch 5/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.6370 - acc: 0.0029 - val_loss: 0.5937 - val_acc: 0.0044\n",
      "Epoch 6/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.6100 - acc: 0.0029 - val_loss: 0.5704 - val_acc: 0.0044\n",
      "Epoch 7/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.5859 - acc: 0.0029 - val_loss: 0.6106 - val_acc: 0.0044\n",
      "Epoch 8/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.5637 - acc: 0.0029 - val_loss: 0.5529 - val_acc: 0.0044\n",
      "Epoch 9/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.5451 - acc: 0.0029 - val_loss: 0.5245 - val_acc: 0.0044\n",
      "Epoch 10/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.5272 - acc: 0.0029 - val_loss: 0.4957 - val_acc: 0.0044\n",
      "Epoch 11/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.5111 - acc: 0.0029 - val_loss: 0.5483 - val_acc: 0.0044\n",
      "Epoch 12/100\n",
      "11610/11610==============================] - 1s 45us/sample - loss: 0.4986 - acc: 0.0029 - val_loss: 0.4929 - val_acc: 0.0044\n",
      "Epoch 13/100\n",
      "11610/11610==============================] - 1s 46us/sample - loss: 0.4862 - acc: 0.0029 - val_loss: 0.4594 - val_acc: 0.0044\n",
      "Epoch 14/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.4753 - acc: 0.0029 - val_loss: 0.4515 - val_acc: 0.0044\n",
      "Epoch 15/100\n",
      "11610/11610==============================] - 0s 42us/sample - loss: 0.4658 - acc: 0.0029 - val_loss: 0.4412 - val_acc: 0.0044\n",
      "Epoch 16/100\n",
      "11610/11610==============================] - 1s 45us/sample - loss: 0.4574 - acc: 0.0029 - val_loss: 0.4323 - val_acc: 0.0044\n",
      "Epoch 17/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4508 - acc: 0.0029 - val_loss: 0.4241 - val_acc: 0.0044\n",
      "Epoch 18/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4450 - acc: 0.0028 - val_loss: 0.4185 - val_acc: 0.0044\n",
      "Epoch 19/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4398 - acc: 0.0028 - val_loss: 0.4173 - val_acc: 0.0044\n",
      "Epoch 20/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4353 - acc: 0.0028 - val_loss: 0.4144 - val_acc: 0.0044\n",
      "Epoch 21/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4316 - acc: 0.0028 - val_loss: 0.4094 - val_acc: 0.0044\n",
      "Epoch 22/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4282 - acc: 0.0028 - val_loss: 0.4146 - val_acc: 0.0044\n",
      "Epoch 23/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4250 - acc: 0.0028 - val_loss: 0.4005 - val_acc: 0.0044\n",
      "Epoch 24/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.4224 - acc: 0.0028 - val_loss: 0.4083 - val_acc: 0.0044\n",
      "Epoch 25/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4197 - acc: 0.0028 - val_loss: 0.4062 - val_acc: 0.0044\n",
      "Epoch 26/100\n",
      "11610/11610==============================] - 0s 42us/sample - loss: 0.4176 - acc: 0.0028 - val_loss: 0.3938 - val_acc: 0.0044\n",
      "Epoch 27/100\n",
      "11610/11610==============================] - 1s 45us/sample - loss: 0.4156 - acc: 0.0028 - val_loss: 0.3930 - val_acc: 0.0044\n",
      "Epoch 28/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4139 - acc: 0.0028 - val_loss: 0.3906 - val_acc: 0.0044\n",
      "Epoch 29/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4121 - acc: 0.0028 - val_loss: 0.4012 - val_acc: 0.0044\n",
      "Epoch 30/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4106 - acc: 0.0028 - val_loss: 0.3954 - val_acc: 0.0044\n",
      "Epoch 31/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.4090 - acc: 0.0028 - val_loss: 0.4050 - val_acc: 0.0044\n",
      "Epoch 32/100\n",
      "11610/11610==============================] - 0s 43us/sample - loss: 0.4079 - acc: 0.0028 - val_loss: 0.3876 - val_acc: 0.0044\n",
      "Epoch 33/100\n",
      "11610/11610==============================] - 1s 44us/sample - loss: 0.4064 - acc: 0.0028 - val_loss: 0.3854 - val_acc: 0.0044\n",
      "Epoch 34/100\n",
      "11610/11610==============================] - 1s 44us/sample - loss: 0.4054 - acc: 0.0028 - val_loss: 0.3866 - val_acc: 0.0044\n",
      "Epoch 35/100\n",
      "11610/11610==============================] - 0s 42us/sample - loss: 0.4041 - acc: 0.0028 - val_loss: 0.4226 - val_acc: 0.0044\n",
      "Epoch 36/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.4034 - acc: 0.0028 - val_loss: 0.3790 - val_acc: 0.0044\n",
      "Epoch 37/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4023 - acc: 0.0028 - val_loss: 0.3884 - val_acc: 0.0044\n",
      "Epoch 38/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4012 - acc: 0.0028 - val_loss: 0.4116 - val_acc: 0.0044\n",
      "Epoch 39/100\n",
      "11610/11610==============================] - 1s 44us/sample - loss: 0.4004 - acc: 0.0028 - val_loss: 0.3944 - val_acc: 0.0044\n",
      "Epoch 40/100\n",
      "11610/11610==============================] - 1s 43us/sample - loss: 0.3994 - acc: 0.0028 - val_loss: 0.3757 - val_acc: 0.0044\n",
      "Epoch 41/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3986 - acc: 0.0028 - val_loss: 0.4187 - val_acc: 0.0044\n",
      "Epoch 42/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.3980 - acc: 0.0028 - val_loss: 0.3737 - val_acc: 0.0044\n",
      "Epoch 43/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.3970 - acc: 0.0028 - val_loss: 0.4078 - val_acc: 0.0044\n",
      "Epoch 44/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3964 - acc: 0.0028 - val_loss: 0.3720 - val_acc: 0.0044\n",
      "Epoch 45/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.3956 - acc: 0.0028 - val_loss: 0.3805 - val_acc: 0.0044\n",
      "Epoch 46/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3950 - acc: 0.0028 - val_loss: 0.3733 - val_acc: 0.0044\n",
      "Epoch 47/100\n",
      "11610/11610==============================] - 1s 46us/sample - loss: 0.3943 - acc: 0.0028 - val_loss: 0.3744 - val_acc: 0.0044\n",
      "Epoch 48/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3936 - acc: 0.0028 - val_loss: 0.4216 - val_acc: 0.0044\n",
      "Epoch 49/100\n",
      "11610/11610==============================] - 0s 27us/sample - loss: 0.3932 - acc: 0.0028 - val_loss: 0.3713 - val_acc: 0.0044\n",
      "Epoch 50/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3924 - acc: 0.0028 - val_loss: 0.4014 - val_acc: 0.0044\n",
      "Epoch 51/100\n",
      "11610/11610==============================] - 0s 31us/sample - loss: 0.3917 - acc: 0.0028 - val_loss: 0.4081 - val_acc: 0.0044\n",
      "Epoch 52/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.3913 - acc: 0.0028 - val_loss: 0.3763 - val_acc: 0.0044\n",
      "Epoch 53/100\n",
      "11610/11610==============================] - 0s 31us/sample - loss: 0.3906 - acc: 0.0028 - val_loss: 0.3692 - val_acc: 0.0044\n",
      "Epoch 54/100\n",
      "11610/11610==============================] - 0s 32us/sample - loss: 0.3900 - acc: 0.0028 - val_loss: 0.3670 - val_acc: 0.0044\n",
      "Epoch 55/100\n",
      "11610/11610==============================] - 0s 30us/sample - loss: 0.3895 - acc: 0.0028 - val_loss: 0.4209 - val_acc: 0.0044\n",
      "Epoch 56/100\n",
      "11610/11610==============================] - 0s 27us/sample - loss: 0.3893 - acc: 0.0028 - val_loss: 0.3771 - val_acc: 0.0044\n",
      "Epoch 57/100\n",
      "11610/11610==============================] - 0s 25us/sample - loss: 0.3886 - acc: 0.0028 - val_loss: 0.3653 - val_acc: 0.0044\n",
      "Epoch 58/100\n",
      "11610/11610==============================] - 0s 25us/sample - loss: 0.3881 - acc: 0.0028 - val_loss: 0.3945 - val_acc: 0.0044\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11610/11610==============================] - 0s 25us/sample - loss: 0.3877 - acc: 0.0028 - val_loss: 0.3634 - val_acc: 0.0044\n",
      "Epoch 60/100\n",
      "11610/11610==============================] - 0s 25us/sample - loss: 0.3871 - acc: 0.0028 - val_loss: 0.3731 - val_acc: 0.0044\n",
      "Epoch 61/100\n",
      "11610/11610==============================] - 0s 28us/sample - loss: 0.3866 - acc: 0.0028 - val_loss: 0.3693 - val_acc: 0.0044\n",
      "Epoch 62/100\n",
      "11610/11610==============================] - 0s 26us/sample - loss: 0.3860 - acc: 0.0028 - val_loss: 0.4206 - val_acc: 0.0044\n",
      "Epoch 63/100\n",
      "11610/11610==============================] - 0s 27us/sample - loss: 0.3857 - acc: 0.0028 - val_loss: 0.3907 - val_acc: 0.0044\n",
      "Epoch 64/100\n",
      "11610/11610==============================] - 0s 25us/sample - loss: 0.3852 - acc: 0.0028 - val_loss: 0.4028 - val_acc: 0.0044\n",
      "5160/5160==============================] - 0s 17us/sample - loss: 0.3822 - acc: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.38219906664633935, 0.002131783]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid),\n",
    "                   callbacks=[\n",
    "                       keras.callbacks.EarlyStopping(patience=5)\n",
    "                   ])\n",
    "\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': [0.0023255814,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0029285098,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772,\n",
      "         0.0028423772],\n",
      " 'loss': [2.0732592611657865,\n",
      "          0.7719293326575832,\n",
      "          0.7069681982558724,\n",
      "          0.6688358237885074,\n",
      "          0.6370185622565198,\n",
      "          0.6099530394276497,\n",
      "          0.5858883197917495,\n",
      "          0.5636935218770786,\n",
      "          0.5451014451175594,\n",
      "          0.5271759131876792,\n",
      "          0.5110505538598717,\n",
      "          0.4985564690792386,\n",
      "          0.48616284111211466,\n",
      "          0.4752536051803165,\n",
      "          0.46577576859874215,\n",
      "          0.4574222175957929,\n",
      "          0.45084665108096694,\n",
      "          0.44500336935559187,\n",
      "          0.43979846782257187,\n",
      "          0.4352803511110284,\n",
      "          0.4315718041936659,\n",
      "          0.42817256944534804,\n",
      "          0.4250085028361699,\n",
      "          0.4223535858896865,\n",
      "          0.4196919006124639,\n",
      "          0.41761849294131836,\n",
      "          0.4155967408167094,\n",
      "          0.41390258116418177,\n",
      "          0.41214523024850624,\n",
      "          0.4106086530323998,\n",
      "          0.408997984983919,\n",
      "          0.4079006460157578,\n",
      "          0.4063503234988138,\n",
      "          0.4053726912776937,\n",
      "          0.40408227102803734,\n",
      "          0.403424346662615,\n",
      "          0.4022976234183036,\n",
      "          0.4011903170416008,\n",
      "          0.40042934842812,\n",
      "          0.3994479380631426,\n",
      "          0.3986054119110929,\n",
      "          0.3980050988591611,\n",
      "          0.3970270516901156,\n",
      "          0.3964448147349887,\n",
      "          0.39561185744895083,\n",
      "          0.3950182440669038,\n",
      "          0.3942733668079672,\n",
      "          0.39355725547961384,\n",
      "          0.39315063100144537,\n",
      "          0.39240713135935334,\n",
      "          0.3917348009722691,\n",
      "          0.3912956263462497,\n",
      "          0.39059694783632143,\n",
      "          0.3899997922595876,\n",
      "          0.3894837665414112,\n",
      "          0.38928277780535303,\n",
      "          0.38858197302165265,\n",
      "          0.388127810671656,\n",
      "          0.3876728763670679,\n",
      "          0.3871406289342146,\n",
      "          0.3866360704463479,\n",
      "          0.38603136161603774,\n",
      "          0.38574269728913174,\n",
      "          0.3852058067442938],\n",
      " 'val_acc': [0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765,\n",
      "             0.004392765],\n",
      " 'val_loss': [1.6606597149402904,\n",
      "              0.6980199671838943,\n",
      "              0.6522552732657401,\n",
      "              0.6654809451380441,\n",
      "              0.5937224065610605,\n",
      "              0.5703978591803125,\n",
      "              0.6105870286931672,\n",
      "              0.5529291770870988,\n",
      "              0.5245132913731173,\n",
      "              0.49566471629056513,\n",
      "              0.5483369108626392,\n",
      "              0.4929343442276159,\n",
      "              0.459394672736333,\n",
      "              0.45152757133932386,\n",
      "              0.4412071793886426,\n",
      "              0.43227734053627775,\n",
      "              0.42409912019737006,\n",
      "              0.41845275976091084,\n",
      "              0.417298524000848,\n",
      "              0.4144163567659467,\n",
      "              0.4094488976383702,\n",
      "              0.41462772527098346,\n",
      "              0.4004617782526238,\n",
      "              0.4083098789672211,\n",
      "              0.4061628439013656,\n",
      "              0.39376205894374106,\n",
      "              0.3930371056847486,\n",
      "              0.39062751418398334,\n",
      "              0.401201412104821,\n",
      "              0.39536019138587536,\n",
      "              0.4049515328974071,\n",
      "              0.38760739734314514,\n",
      "              0.38535182174170046,\n",
      "              0.3865940291136118,\n",
      "              0.42261029881562373,\n",
      "              0.37901963556151674,\n",
      "              0.38840128758618997,\n",
      "              0.4115540885802079,\n",
      "              0.39436307170594387,\n",
      "              0.3756933310851262,\n",
      "              0.41866092062735744,\n",
      "              0.3736597671373254,\n",
      "              0.40784198898985713,\n",
      "              0.3720204219491599,\n",
      "              0.38045744276786037,\n",
      "              0.37329628864924114,\n",
      "              0.37443445532820946,\n",
      "              0.42155933390908157,\n",
      "              0.37125417299048846,\n",
      "              0.40141060395450245,\n",
      "              0.40810192553879987,\n",
      "              0.37632363030460764,\n",
      "              0.3691866234137414,\n",
      "              0.3669907362424126,\n",
      "              0.4209211906105357,\n",
      "              0.3771346191642204,\n",
      "              0.36526023403618685,\n",
      "              0.39447097761378425,\n",
      "              0.363448787805954,\n",
      "              0.3731228034933716,\n",
      "              0.36932864021269235,\n",
      "              0.42062755248651335,\n",
      "              0.3906983496221769,\n",
      "              0.4028199526690697]}\n"
     ]
    }
   ],
   "source": [
    "pprint(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) If you want your code to be portable to other Python implementations of the Keras API, you should use the operations in `keras.backend` rather than TensorFlow operations directly. This package contains thin wrappers around the backend's operations (for example, `keras.backend.square()` simply calls `tf.square()`). Try reimplementing the `my_mse()` function this way and use it to train and evaluate your model again. **Tip**: people frequently define `K = keras.backend` to make their code more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "False\n",
      "tf.Tensor(True, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "#assert my_mse(tf.constant([[1., 2., 3.], [4., 5., 6.]]), tf.constant([[1., 2., 3.], [4., 5., 6.]])) == 0.0\n",
    "\n",
    "print(my_mse(tf.constant([[1., 2., 3.], [4., 5., 6.]]), tf.constant([[1., 2., 3.], [4., 5., 6.]])))\n",
    "print(tf.constant(0.0))\n",
    "print(my_mse(tf.constant([[1., 2., 3.], [4., 5., 6.]]), tf.constant([[1., 2., 3.], [4., 5., 6.]])) == \n",
    "      tf.constant(0.0))\n",
    "print(tf.equal(x=my_mse(tf.constant([[1., 2., 3.], [4., 5., 6.]]), tf.constant([[1., 2., 3.], [4., 5., 6.]])),\n",
    "        y=tf.constant(0.0)))\n",
    "\n",
    "tf.debugging.assert_equal(\n",
    "    x=my_mse(tf.constant([[1., 2., 3.], [4., 5., 6.]]), tf.constant([[1., 2., 3.], [4., 5., 6.]])),\n",
    "    y=tf.constant(0.0))\n",
    "\n",
    "tf.debugging.assert_equal(\n",
    "    x=my_mse(tf.constant([[1., 2., 3.], [4., 5., 6.]]), tf.constant([[1., 2., 3.5], [4., 5., 5.5]])),\n",
    "    y=tf.constant(0.083333336))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Create an `my_mse()` function with two arguments: the true labels `y_true` and the model predictions `y_pred`. Make it return the mean squared error using TensorFlow operations. Note that you could write your own custom metrics in exactly the same way. **Tip**: recall that the MSE is the mean of the squares of prediction errors, which are the differences between the predictions and the labels, so you will need to use `tf.reduce_mean()` and `tf.square()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Compile your model, passing it your custom loss function, then train it and evaluate it. **Tip**: don't forget to use the scaled sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_mse, optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Try building and compiling the model again, this time adding `\"mse\"` (or equivalently `\"mean_squared_error\"` or `keras.losses.mean_squared_error`) to the list of additional metrics, then train the model and make sure the `my_mse` is equal to the standard `mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=my_mse, optimizer=\"sgd\", metrics=[\"mean_squared_error\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) If you want your code to be portable to other Python implementations of the Keras API, you should use the operations in `keras.backend` rather than TensorFlow operations directly. This package contains thin wrappers around the backend's operations (for example, `keras.backend.square()` simply calls `tf.square()`). Try reimplementing the `my_mse()` function this way and use it to train and evaluate your model again. **Tip**: people frequently define `K = keras.backend` to make their code more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_portable_mse(y_true, y_pred):\n",
    "    K = keras.backend\n",
    "    return K.mean(K.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=my_portable_mse, optimizer=\"sgd\", metrics=[\"mean_squared_error\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 – Custom layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Some layers have no weights, such as `keras.layers.Flatten` or `keras.layers.ReLU`. If you want to create a custom layer without any weights, the simplest option is to create a `keras.layers.Lambda` layer and pass it the function to perform. For example, try creating a custom layer that applies the softplus function (log(exp(X) + 1), and try calling this layer like a regular function. **Tip**: you can use `tf.math.softplus()` rather than computing the log and the exponential manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "? keras.layers.Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_custom_layer = keras.layers.Lambda(function=lambda X: tf.math.softplus(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Lambda at 0x13ab5c1d0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_custom_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Create a regression model like in exercise 1, but add your softplus layer at the top (i.e., after the existing 1-unit dense layer). This can be useful to ensure that your model never predicts negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    return keras.models.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", input_shape=X_train_scaled.shape[1:], name=\"dense_30\"),\n",
    "        keras.layers.Dense(1, name=\"dense_1\"),\n",
    "        keras.layers.Lambda(function=tf.math.softplus, name=\"softplus_layer\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 31        \n",
      "_________________________________________________________________\n",
      "softplus_layer (Lambda)      (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "initial_weights = model.get_weights()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_mse, optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610==============================] - 1s 45us/sample - loss: 1.9873 - acc: 0.0028 - val_loss: 3.0341 - val_acc: 0.0044\n",
      "Epoch 2/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.9619 - acc: 0.0029 - val_loss: 0.7737 - val_acc: 0.0044\n",
      "Epoch 3/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.7866 - acc: 0.0029 - val_loss: 0.6894 - val_acc: 0.0044\n",
      "Epoch 4/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.7193 - acc: 0.0029 - val_loss: 0.6422 - val_acc: 0.0044\n",
      "Epoch 5/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.6757 - acc: 0.0029 - val_loss: 0.6086 - val_acc: 0.0044\n",
      "Epoch 6/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.6424 - acc: 0.0029 - val_loss: 0.5819 - val_acc: 0.0044\n",
      "Epoch 7/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.6151 - acc: 0.0029 - val_loss: 0.5583 - val_acc: 0.0044\n",
      "Epoch 8/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.5916 - acc: 0.0029 - val_loss: 0.5379 - val_acc: 0.0044\n",
      "Epoch 9/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.5708 - acc: 0.0029 - val_loss: 0.5198 - val_acc: 0.0044\n",
      "Epoch 10/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.5529 - acc: 0.0029 - val_loss: 0.5045 - val_acc: 0.0044\n",
      "Epoch 11/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.5372 - acc: 0.0029 - val_loss: 0.4906 - val_acc: 0.0044\n",
      "Epoch 12/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.5234 - acc: 0.0029 - val_loss: 0.4785 - val_acc: 0.0044\n",
      "Epoch 13/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.5113 - acc: 0.0029 - val_loss: 0.4677 - val_acc: 0.0044\n",
      "Epoch 14/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.5007 - acc: 0.0029 - val_loss: 0.4581 - val_acc: 0.0044\n",
      "Epoch 15/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4911 - acc: 0.0029 - val_loss: 0.4497 - val_acc: 0.0044\n",
      "Epoch 16/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4828 - acc: 0.0029 - val_loss: 0.4422 - val_acc: 0.0044\n",
      "Epoch 17/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4752 - acc: 0.0029 - val_loss: 0.4354 - val_acc: 0.0044\n",
      "Epoch 18/100\n",
      "11610/11610==============================] - 0s 32us/sample - loss: 0.4685 - acc: 0.0029 - val_loss: 0.4291 - val_acc: 0.0044\n",
      "Epoch 19/100\n",
      "11610/11610==============================] - 0s 32us/sample - loss: 0.4625 - acc: 0.0029 - val_loss: 0.4237 - val_acc: 0.0044\n",
      "Epoch 20/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4570 - acc: 0.0029 - val_loss: 0.4189 - val_acc: 0.0044\n",
      "Epoch 21/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4521 - acc: 0.0029 - val_loss: 0.4145 - val_acc: 0.0044\n",
      "Epoch 22/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4478 - acc: 0.0029 - val_loss: 0.4105 - val_acc: 0.0044\n",
      "Epoch 23/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4439 - acc: 0.0029 - val_loss: 0.4072 - val_acc: 0.0044\n",
      "Epoch 24/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4401 - acc: 0.0029 - val_loss: 0.4043 - val_acc: 0.0044\n",
      "Epoch 25/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4370 - acc: 0.0029 - val_loss: 0.4013 - val_acc: 0.0044\n",
      "Epoch 26/100\n",
      "11610/11610==============================] - 0s 33us/sample - loss: 0.4338 - acc: 0.0029 - val_loss: 0.3987 - val_acc: 0.0044\n",
      "Epoch 27/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4311 - acc: 0.0029 - val_loss: 0.3966 - val_acc: 0.0044\n",
      "Epoch 28/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4286 - acc: 0.0029 - val_loss: 0.3946 - val_acc: 0.0044\n",
      "Epoch 29/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.4262 - acc: 0.0029 - val_loss: 0.3927 - val_acc: 0.0044\n",
      "Epoch 30/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4240 - acc: 0.0029 - val_loss: 0.3913 - val_acc: 0.0044\n",
      "Epoch 31/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.4220 - acc: 0.0029 - val_loss: 0.3895 - val_acc: 0.0044\n",
      "Epoch 32/100\n",
      "11610/11610==============================] - 1s 44us/sample - loss: 0.4201 - acc: 0.0029 - val_loss: 0.3879 - val_acc: 0.0044\n",
      "Epoch 33/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.4182 - acc: 0.0029 - val_loss: 0.3865 - val_acc: 0.0044\n",
      "Epoch 34/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4165 - acc: 0.0029 - val_loss: 0.3854 - val_acc: 0.0044\n",
      "Epoch 35/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4149 - acc: 0.0029 - val_loss: 0.3839 - val_acc: 0.0044\n",
      "Epoch 36/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4134 - acc: 0.0029 - val_loss: 0.3822 - val_acc: 0.0044\n",
      "Epoch 37/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4119 - acc: 0.0029 - val_loss: 0.3813 - val_acc: 0.0044\n",
      "Epoch 38/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4104 - acc: 0.0029 - val_loss: 0.3803 - val_acc: 0.0044\n",
      "Epoch 39/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4090 - acc: 0.0029 - val_loss: 0.3789 - val_acc: 0.0044\n",
      "Epoch 40/100\n",
      "11610/11610==============================] - 0s 24us/sample - loss: 0.4076 - acc: 0.0029 - val_loss: 0.3779 - val_acc: 0.0044\n",
      "Epoch 41/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4064 - acc: 0.0029 - val_loss: 0.3770 - val_acc: 0.0044\n",
      "Epoch 42/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4050 - acc: 0.0029 - val_loss: 0.3761 - val_acc: 0.0044\n",
      "Epoch 43/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4039 - acc: 0.0029 - val_loss: 0.3752 - val_acc: 0.0044\n",
      "Epoch 44/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.4027 - acc: 0.0029 - val_loss: 0.3741 - val_acc: 0.0044\n",
      "Epoch 45/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4016 - acc: 0.0029 - val_loss: 0.3731 - val_acc: 0.0044\n",
      "Epoch 46/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4005 - acc: 0.0029 - val_loss: 0.3722 - val_acc: 0.0044\n",
      "Epoch 47/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3994 - acc: 0.0029 - val_loss: 0.3713 - val_acc: 0.0044\n",
      "Epoch 48/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3985 - acc: 0.0029 - val_loss: 0.3705 - val_acc: 0.0044\n",
      "Epoch 49/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3974 - acc: 0.0029 - val_loss: 0.3699 - val_acc: 0.0044\n",
      "Epoch 50/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3965 - acc: 0.0029 - val_loss: 0.3691 - val_acc: 0.0044\n",
      "Epoch 51/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.3955 - acc: 0.0029 - val_loss: 0.3682 - val_acc: 0.0044\n",
      "Epoch 52/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3946 - acc: 0.0029 - val_loss: 0.3674 - val_acc: 0.0044\n",
      "Epoch 53/100\n",
      "11610/11610==============================] - 0s 32us/sample - loss: 0.3937 - acc: 0.0029 - val_loss: 0.3666 - val_acc: 0.0044\n",
      "Epoch 54/100\n",
      "11610/11610==============================] - 0s 30us/sample - loss: 0.3927 - acc: 0.0029 - val_loss: 0.3659 - val_acc: 0.0044\n",
      "Epoch 55/100\n",
      "11610/11610==============================] - 0s 42us/sample - loss: 0.3919 - acc: 0.0029 - val_loss: 0.3651 - val_acc: 0.0044\n",
      "Epoch 56/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.3911 - acc: 0.0029 - val_loss: 0.3647 - val_acc: 0.0044\n",
      "Epoch 57/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3903 - acc: 0.0029 - val_loss: 0.3639 - val_acc: 0.0044\n",
      "Epoch 58/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3895 - acc: 0.0029 - val_loss: 0.3630 - val_acc: 0.0044\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3886 - acc: 0.0029 - val_loss: 0.3623 - val_acc: 0.0044\n",
      "Epoch 60/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.3877 - acc: 0.0029 - val_loss: 0.3617 - val_acc: 0.0044\n",
      "Epoch 61/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.3870 - acc: 0.0029 - val_loss: 0.3611 - val_acc: 0.0044\n",
      "Epoch 62/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3863 - acc: 0.0029 - val_loss: 0.3602 - val_acc: 0.0044\n",
      "Epoch 63/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3854 - acc: 0.0029 - val_loss: 0.3598 - val_acc: 0.0044\n",
      "Epoch 64/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3845 - acc: 0.0029 - val_loss: 0.3593 - val_acc: 0.0044\n",
      "Epoch 65/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.3839 - acc: 0.0029 - val_loss: 0.3584 - val_acc: 0.0044\n",
      "Epoch 66/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3831 - acc: 0.0029 - val_loss: 0.3578 - val_acc: 0.0044\n",
      "Epoch 67/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.3824 - acc: 0.0029 - val_loss: 0.3574 - val_acc: 0.0044\n",
      "Epoch 68/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.3817 - acc: 0.0029 - val_loss: 0.3569 - val_acc: 0.0044\n",
      "Epoch 69/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3808 - acc: 0.0029 - val_loss: 0.3564 - val_acc: 0.0044\n",
      "Epoch 70/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3802 - acc: 0.0029 - val_loss: 0.3554 - val_acc: 0.0044\n",
      "Epoch 71/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3794 - acc: 0.0029 - val_loss: 0.3550 - val_acc: 0.0044\n",
      "Epoch 72/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.3787 - acc: 0.0029 - val_loss: 0.3542 - val_acc: 0.0044\n",
      "Epoch 73/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3779 - acc: 0.0029 - val_loss: 0.3539 - val_acc: 0.0044\n",
      "Epoch 74/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.3771 - acc: 0.0029 - val_loss: 0.3533 - val_acc: 0.0044\n",
      "Epoch 75/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.3763 - acc: 0.0029 - val_loss: 0.3527 - val_acc: 0.0044\n",
      "Epoch 76/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3756 - acc: 0.0029 - val_loss: 0.3521 - val_acc: 0.0044\n",
      "Epoch 77/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3749 - acc: 0.0029 - val_loss: 0.3516 - val_acc: 0.0044\n",
      "Epoch 78/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3741 - acc: 0.0029 - val_loss: 0.3511 - val_acc: 0.0044\n",
      "Epoch 79/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3732 - acc: 0.0029 - val_loss: 0.3509 - val_acc: 0.0044\n",
      "Epoch 80/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3726 - acc: 0.0029 - val_loss: 0.3500 - val_acc: 0.0044\n",
      "Epoch 81/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3717 - acc: 0.0029 - val_loss: 0.3498 - val_acc: 0.0044\n",
      "Epoch 82/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.3711 - acc: 0.0029 - val_loss: 0.3490 - val_acc: 0.0044\n",
      "Epoch 83/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3701 - acc: 0.0029 - val_loss: 0.3483 - val_acc: 0.0044\n",
      "Epoch 84/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.3692 - acc: 0.0029 - val_loss: 0.3571 - val_acc: 0.0044\n",
      "Epoch 85/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.3683 - acc: 0.0029 - val_loss: 0.3586 - val_acc: 0.0044\n",
      "Epoch 86/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3678 - acc: 0.0029 - val_loss: 0.3701 - val_acc: 0.0044\n",
      "Epoch 87/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.3672 - acc: 0.0029 - val_loss: 0.3914 - val_acc: 0.0044\n",
      "Epoch 88/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3668 - acc: 0.0029 - val_loss: 0.3683 - val_acc: 0.0044\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid),\n",
    "                   callbacks=[\n",
    "                       keras.callbacks.EarlyStopping(patience=5)\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160==============================] - 0s 12us/sample - loss: 0.3630 - acc: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3629901480305102, 0.002131783]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Alternatively, try using this softplus layer as the activation function of the output layer.\n",
    "\n",
    "**Notes**:\n",
    "* setting a layer's activation function is just a handy way of adding an extra weightless layer.\n",
    "* Keras supports the softplus activation function out of the box:\n",
    "  * set `activation=\"softplus\"`\n",
    "  * or set `activation=keras.activations.softplus`\n",
    "  * or add a `keras.layers.Activation(\"softplus\")` layer to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    return keras.models.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", input_shape=X_train_scaled.shape[1:], name=\"dense_30\"),\n",
    "        keras.layers.Dense(1, name=\"dense_1\", activation=\"softplus\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "initial_weights = model.get_weights()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610==============================] - 1s 45us/sample - loss: 2.0066 - acc: 0.0029 - val_loss: 1.0028 - val_acc: 0.0044\n",
      "Epoch 2/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.8450 - acc: 0.0029 - val_loss: 0.7134 - val_acc: 0.0044\n",
      "Epoch 3/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.7157 - acc: 0.0029 - val_loss: 0.6615 - val_acc: 0.0044\n",
      "Epoch 4/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.6707 - acc: 0.0029 - val_loss: 0.6273 - val_acc: 0.0044\n",
      "Epoch 5/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.6373 - acc: 0.0029 - val_loss: 0.5985 - val_acc: 0.0044\n",
      "Epoch 6/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.6092 - acc: 0.0029 - val_loss: 0.5730 - val_acc: 0.0044\n",
      "Epoch 7/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.5850 - acc: 0.0029 - val_loss: 0.5509 - val_acc: 0.0044\n",
      "Epoch 8/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.5636 - acc: 0.0029 - val_loss: 0.5310 - val_acc: 0.0044\n",
      "Epoch 9/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.5450 - acc: 0.0029 - val_loss: 0.5130 - val_acc: 0.0044\n",
      "Epoch 10/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.5283 - acc: 0.0029 - val_loss: 0.4968 - val_acc: 0.0044\n",
      "Epoch 11/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.5135 - acc: 0.0029 - val_loss: 0.4825 - val_acc: 0.0044\n",
      "Epoch 12/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.5006 - acc: 0.0029 - val_loss: 0.4688 - val_acc: 0.0044\n",
      "Epoch 13/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4892 - acc: 0.0029 - val_loss: 0.4589 - val_acc: 0.0044\n",
      "Epoch 14/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4790 - acc: 0.0029 - val_loss: 0.4479 - val_acc: 0.0044\n",
      "Epoch 15/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4708 - acc: 0.0029 - val_loss: 0.4394 - val_acc: 0.0044\n",
      "Epoch 16/100\n",
      "11610/11610==============================] - 0s 43us/sample - loss: 0.4632 - acc: 0.0029 - val_loss: 0.4317 - val_acc: 0.0044\n",
      "Epoch 17/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4565 - acc: 0.0029 - val_loss: 0.4260 - val_acc: 0.0044\n",
      "Epoch 18/100\n",
      "11610/11610==============================] - 1s 47us/sample - loss: 0.4510 - acc: 0.0029 - val_loss: 0.4199 - val_acc: 0.0044\n",
      "Epoch 19/100\n",
      "11610/11610==============================] - 0s 31us/sample - loss: 0.4455 - acc: 0.0029 - val_loss: 0.4166 - val_acc: 0.0044\n",
      "Epoch 20/100\n",
      "11610/11610==============================] - 0s 32us/sample - loss: 0.4412 - acc: 0.0029 - val_loss: 0.4100 - val_acc: 0.0044\n",
      "Epoch 21/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4373 - acc: 0.0029 - val_loss: 0.4060 - val_acc: 0.0044\n",
      "Epoch 22/100\n",
      "11610/11610==============================] - 0s 43us/sample - loss: 0.4339 - acc: 0.0029 - val_loss: 0.4022 - val_acc: 0.0044\n",
      "Epoch 23/100\n",
      "11610/11610==============================] - 0s 31us/sample - loss: 0.4304 - acc: 0.0029 - val_loss: 0.3989 - val_acc: 0.0044\n",
      "Epoch 24/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.4279 - acc: 0.0029 - val_loss: 0.3964 - val_acc: 0.0044\n",
      "Epoch 25/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4253 - acc: 0.0029 - val_loss: 0.3943 - val_acc: 0.0044\n",
      "Epoch 26/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4226 - acc: 0.0029 - val_loss: 0.3915 - val_acc: 0.0044\n",
      "Epoch 27/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4209 - acc: 0.0029 - val_loss: 0.3897 - val_acc: 0.0044\n",
      "Epoch 28/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.4187 - acc: 0.0029 - val_loss: 0.3878 - val_acc: 0.0044\n",
      "Epoch 29/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4168 - acc: 0.0029 - val_loss: 0.3858 - val_acc: 0.0044\n",
      "Epoch 30/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.4150 - acc: 0.0029 - val_loss: 0.3844 - val_acc: 0.0044\n",
      "Epoch 31/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4135 - acc: 0.0029 - val_loss: 0.3824 - val_acc: 0.0044\n",
      "Epoch 32/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4116 - acc: 0.0029 - val_loss: 0.3811 - val_acc: 0.0044\n",
      "Epoch 33/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4103 - acc: 0.0029 - val_loss: 0.3800 - val_acc: 0.0044\n",
      "Epoch 34/100\n",
      "11610/11610==============================] - 0s 33us/sample - loss: 0.4088 - acc: 0.0029 - val_loss: 0.3787 - val_acc: 0.0044\n",
      "Epoch 35/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4076 - acc: 0.0029 - val_loss: 0.3774 - val_acc: 0.0044\n",
      "Epoch 36/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4063 - acc: 0.0029 - val_loss: 0.3768 - val_acc: 0.0044\n",
      "Epoch 37/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.4051 - acc: 0.0029 - val_loss: 0.3753 - val_acc: 0.0044\n",
      "Epoch 38/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.4035 - acc: 0.0029 - val_loss: 0.3735 - val_acc: 0.0044\n",
      "Epoch 39/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4027 - acc: 0.0029 - val_loss: 0.3724 - val_acc: 0.0044\n",
      "Epoch 40/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4018 - acc: 0.0029 - val_loss: 0.3717 - val_acc: 0.0044\n",
      "Epoch 41/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.4008 - acc: 0.0029 - val_loss: 0.3707 - val_acc: 0.0044\n",
      "Epoch 42/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 0.3995 - acc: 0.0029 - val_loss: 0.3700 - val_acc: 0.0044\n",
      "Epoch 43/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.3985 - acc: 0.0029 - val_loss: 0.3691 - val_acc: 0.0044\n",
      "Epoch 44/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.3978 - acc: 0.0029 - val_loss: 0.3683 - val_acc: 0.0044\n",
      "Epoch 45/100\n",
      "11610/11610==============================] - 0s 24us/sample - loss: 0.3965 - acc: 0.0029 - val_loss: 0.3672 - val_acc: 0.0044\n",
      "Epoch 46/100\n",
      "11610/11610==============================] - 0s 32us/sample - loss: 0.3959 - acc: 0.0029 - val_loss: 0.3669 - val_acc: 0.0044\n",
      "Epoch 47/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.3949 - acc: 0.0029 - val_loss: 0.3654 - val_acc: 0.0044\n",
      "Epoch 48/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.3938 - acc: 0.0029 - val_loss: 0.3650 - val_acc: 0.0044\n",
      "Epoch 49/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.3928 - acc: 0.0029 - val_loss: 0.3640 - val_acc: 0.0044\n",
      "Epoch 50/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.3927 - acc: 0.0029 - val_loss: 0.3635 - val_acc: 0.0044\n",
      "Epoch 51/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3915 - acc: 0.0029 - val_loss: 0.3634 - val_acc: 0.0044\n",
      "Epoch 52/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3911 - acc: 0.0029 - val_loss: 0.3627 - val_acc: 0.0044\n",
      "Epoch 53/100\n",
      "11610/11610==============================] - 0s 42us/sample - loss: 0.3904 - acc: 0.0029 - val_loss: 0.3619 - val_acc: 0.0044\n",
      "Epoch 54/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3893 - acc: 0.0029 - val_loss: 0.3615 - val_acc: 0.0044\n",
      "Epoch 55/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3889 - acc: 0.0029 - val_loss: 0.3605 - val_acc: 0.0044\n",
      "Epoch 56/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3881 - acc: 0.0029 - val_loss: 0.3600 - val_acc: 0.0044\n",
      "Epoch 57/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3876 - acc: 0.0029 - val_loss: 0.3599 - val_acc: 0.0044\n",
      "Epoch 58/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.3865 - acc: 0.0029 - val_loss: 0.3592 - val_acc: 0.0044\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11610/11610==============================] - 0s 40us/sample - loss: 0.3860 - acc: 0.0029 - val_loss: 0.3598 - val_acc: 0.0044\n",
      "Epoch 60/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.3850 - acc: 0.0029 - val_loss: 0.3583 - val_acc: 0.0044\n",
      "Epoch 61/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3848 - acc: 0.0029 - val_loss: 0.3586 - val_acc: 0.0044\n",
      "Epoch 62/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3841 - acc: 0.0029 - val_loss: 0.3575 - val_acc: 0.0044\n",
      "Epoch 63/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3836 - acc: 0.0029 - val_loss: 0.3576 - val_acc: 0.0044\n",
      "Epoch 64/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3832 - acc: 0.0029 - val_loss: 0.3566 - val_acc: 0.0044\n",
      "Epoch 65/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.3821 - acc: 0.0029 - val_loss: 0.3560 - val_acc: 0.0044\n",
      "Epoch 66/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3818 - acc: 0.0029 - val_loss: 0.3552 - val_acc: 0.0044\n",
      "Epoch 67/100\n",
      "11610/11610==============================] - 1s 46us/sample - loss: 0.3812 - acc: 0.0029 - val_loss: 0.3552 - val_acc: 0.0044\n",
      "Epoch 68/100\n",
      "11610/11610==============================] - 1s 47us/sample - loss: 0.3804 - acc: 0.0029 - val_loss: 0.3550 - val_acc: 0.0044\n",
      "Epoch 69/100\n",
      "11610/11610==============================] - 1s 52us/sample - loss: 0.3801 - acc: 0.0029 - val_loss: 0.3545 - val_acc: 0.0044\n",
      "Epoch 70/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3790 - acc: 0.0029 - val_loss: 0.3540 - val_acc: 0.0044\n",
      "Epoch 71/100\n",
      "11610/11610==============================] - 0s 27us/sample - loss: 0.3792 - acc: 0.0029 - val_loss: 0.3534 - val_acc: 0.0044\n",
      "Epoch 72/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.3780 - acc: 0.0029 - val_loss: 0.3527 - val_acc: 0.0044\n",
      "Epoch 73/100\n",
      "11610/11610==============================] - 0s 33us/sample - loss: 0.3778 - acc: 0.0029 - val_loss: 0.3529 - val_acc: 0.0044\n",
      "Epoch 74/100\n",
      "11610/11610==============================] - 0s 31us/sample - loss: 0.3771 - acc: 0.0029 - val_loss: 0.3523 - val_acc: 0.0044\n",
      "Epoch 75/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.3760 - acc: 0.0029 - val_loss: 0.3529 - val_acc: 0.0044\n",
      "Epoch 76/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.3762 - acc: 0.0029 - val_loss: 0.3518 - val_acc: 0.0044\n",
      "Epoch 77/100\n",
      "11610/11610==============================] - 1s 45us/sample - loss: 0.3755 - acc: 0.0029 - val_loss: 0.3512 - val_acc: 0.0044\n",
      "Epoch 78/100\n",
      "11610/11610==============================] - 0s 29us/sample - loss: 0.3748 - acc: 0.0029 - val_loss: 0.3504 - val_acc: 0.0044\n",
      "Epoch 79/100\n",
      "11610/11610==============================] - 0s 25us/sample - loss: 0.3738 - acc: 0.0029 - val_loss: 0.3488 - val_acc: 0.0044\n",
      "Epoch 80/100\n",
      "11610/11610==============================] - 0s 26us/sample - loss: 0.3739 - acc: 0.0029 - val_loss: 0.3528 - val_acc: 0.0044\n",
      "Epoch 81/100\n",
      "11610/11610==============================] - 0s 30us/sample - loss: 0.3734 - acc: 0.0029 - val_loss: 0.3586 - val_acc: 0.0044\n",
      "Epoch 82/100\n",
      "11610/11610==============================] - 0s 28us/sample - loss: 0.3728 - acc: 0.0029 - val_loss: 0.3665 - val_acc: 0.0044\n",
      "Epoch 83/100\n",
      "11610/11610==============================] - 0s 27us/sample - loss: 0.3723 - acc: 0.0029 - val_loss: 0.3763 - val_acc: 0.0044\n",
      "Epoch 84/100\n",
      "11610/11610==============================] - 0s 29us/sample - loss: 0.3719 - acc: 0.0029 - val_loss: 0.3785 - val_acc: 0.0044\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=my_mse, optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid),\n",
    "                   callbacks=[\n",
    "                       keras.callbacks.EarlyStopping(patience=5)\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160==============================] - 0s 13us/sample - loss: 0.3659 - acc: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36593824151412463, 0.002131783]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curves(history)\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Now let's create a custom layer with its own weights. Use the following template to create a `MyDense` layer that computes $\\phi(\\mathbf{X} \\mathbf{W}) + \\mathbf{b}$, where $\\phi$ is the (optional) activation function, $\\mathbf{X}$ is the input data, $\\mathbf{W}$ represents the kernel (i.e., connection weights), and $\\mathbf{b}$ represents the biases, then train and evaluate a model using this instead of a regular `Dense` layer.\n",
    "\n",
    "**Tips**:\n",
    "* The constructor `__init__()`:\n",
    "  * It must have all your layer's hyperparameters as arguments, and save them to instance variables. You will need the number of `units` and the optional `activation` function. To support all kinds of activation functions (strings or functions), simply create a `keras.layers.Activation` passing it the `activation` argument.\n",
    "  * The `**kwargs` argument must be passed to the base class's constructor (`super().__init__()`) so your class can support the `input_shape` argument, and more.\n",
    "* The `build()` method:\n",
    "  * The `build()` method will be called automatically by Keras when it knows the shape of the inputs. Note that the argument should really be called `batch_input_shape` since it includes the batch size.\n",
    "  * You must call `self.add_weight()` for each weight you want to create, specifying its `name`, `shape` (which often depends on the `input_shape`), how to initialize it, and whether or not it is `trainable`. You need two weights: the `kernel` (connection weights) and the `biases`. The kernel must be initialized randomly. The biases are usually initialized with zeros. **Note**: you can find many initializers in `keras.initializers`.\n",
    "  * Do not forget to call `super().build()`, so Keras knows that the model has been built.\n",
    "  * Note: you could create the weights in the constructor, but it is preferable to create them in the `build()` method, because users of your class may not always know the `input_shape` when creating the model. The first time the model is used on some actual data, the `build()` method will automatically be called with the actual `input_shape`.\n",
    "* The `call()` method:\n",
    "  * This is where to code your layer's actual computations. As before, you can use TensorFlow operations directly, or use `keras.backend` operations if you want the layer to be portable to other Keras implementations.\n",
    "* The `compute_output_shape()` method:\n",
    "  * You do not need to implement this method when using tf.keras, as the `Layer` class provides a good implementation.\n",
    "  * However, if want to port your code to another Keras implementation (such as keras-team), and if the output shape is different from the input shape, then you need to implement this method. Note that the input shape is actually the batch input shape, and the ouptut shape must be the batch output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This template was copied from https://keras.io/layers/writing-your-own-keras-layers/\n",
    "# I just removed the imports and replaced Layer with keras.layers.Layer.\n",
    "\n",
    "class MyLayer(keras.layers.Layer):\n",
    "    def __init__(self, output_dim, activation, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        \n",
    "        self.biases = self.add_weight(name='bias', \n",
    "                                      shape=(self.output_dim,),\n",
    "                                      initializer='zeros',\n",
    "                                      trainable=True)\n",
    "        \n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.activation(K.dot(x, self.kernel) + self.biases)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    return keras.models.Sequential([\n",
    "        MyLayer(output_dim=30, activation=\"relu\", input_shape=X_train_scaled.shape[1:], name=\"my_dense_30\"),\n",
    "        MyLayer(output_dim=1, activation=\"relu\", name=\"my_dense_1\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "my_dense_30 (MyLayer)        (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "my_dense_1 (MyLayer)         (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "initial_weights = model.get_weights()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610==============================] - 1s 47us/sample - loss: 3.5388 - acc: 0.0022 - val_loss: 2.0971 - val_acc: 0.0044\n",
      "Epoch 2/100\n",
      "11610/11610==============================] - 0s 34us/sample - loss: 1.6166 - acc: 0.0029 - val_loss: 1.2017 - val_acc: 0.0044\n",
      "Epoch 3/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 1.0082 - acc: 0.0029 - val_loss: 0.8684 - val_acc: 0.0044\n",
      "Epoch 4/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.7844 - acc: 0.0029 - val_loss: 0.7235 - val_acc: 0.0044\n",
      "Epoch 5/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.7011 - acc: 0.0029 - val_loss: 0.6520 - val_acc: 0.0044\n",
      "Epoch 6/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.6605 - acc: 0.0029 - val_loss: 0.6142 - val_acc: 0.0044\n",
      "Epoch 7/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.6344 - acc: 0.0029 - val_loss: 0.5895 - val_acc: 0.0044\n",
      "Epoch 8/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.6138 - acc: 0.0029 - val_loss: 0.5695 - val_acc: 0.0044\n",
      "Epoch 9/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.5959 - acc: 0.0029 - val_loss: 0.5522 - val_acc: 0.0044\n",
      "Epoch 10/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.5793 - acc: 0.0029 - val_loss: 0.5361 - val_acc: 0.0044\n",
      "Epoch 11/100\n",
      "11610/11610==============================] - 0s 32us/sample - loss: 0.5636 - acc: 0.0029 - val_loss: 0.5210 - val_acc: 0.0044\n",
      "Epoch 12/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.5488 - acc: 0.0029 - val_loss: 0.5069 - val_acc: 0.0044\n",
      "Epoch 13/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.5354 - acc: 0.0029 - val_loss: 0.4940 - val_acc: 0.0044\n",
      "Epoch 14/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.5228 - acc: 0.0029 - val_loss: 0.4820 - val_acc: 0.0044\n",
      "Epoch 15/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.5116 - acc: 0.0029 - val_loss: 0.4710 - val_acc: 0.0044\n",
      "Epoch 16/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.5014 - acc: 0.0029 - val_loss: 0.4614 - val_acc: 0.0044\n",
      "Epoch 17/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4922 - acc: 0.0029 - val_loss: 0.4527 - val_acc: 0.0044\n",
      "Epoch 18/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4842 - acc: 0.0029 - val_loss: 0.4446 - val_acc: 0.0044\n",
      "Epoch 19/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.4773 - acc: 0.0029 - val_loss: 0.4380 - val_acc: 0.0044\n",
      "Epoch 20/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.4711 - acc: 0.0029 - val_loss: 0.4323 - val_acc: 0.0044\n",
      "Epoch 21/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4657 - acc: 0.0029 - val_loss: 0.4269 - val_acc: 0.0044\n",
      "Epoch 22/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4612 - acc: 0.0029 - val_loss: 0.4225 - val_acc: 0.0044\n",
      "Epoch 23/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4572 - acc: 0.0029 - val_loss: 0.4186 - val_acc: 0.0044\n",
      "Epoch 24/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4537 - acc: 0.0029 - val_loss: 0.4153 - val_acc: 0.0044\n",
      "Epoch 25/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4507 - acc: 0.0029 - val_loss: 0.4125 - val_acc: 0.0044\n",
      "Epoch 26/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4482 - acc: 0.0029 - val_loss: 0.4100 - val_acc: 0.0044\n",
      "Epoch 27/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4459 - acc: 0.0029 - val_loss: 0.4077 - val_acc: 0.0044\n",
      "Epoch 28/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4438 - acc: 0.0029 - val_loss: 0.4061 - val_acc: 0.0044\n",
      "Epoch 29/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4420 - acc: 0.0029 - val_loss: 0.4042 - val_acc: 0.0044\n",
      "Epoch 30/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4403 - acc: 0.0029 - val_loss: 0.4025 - val_acc: 0.0044\n",
      "Epoch 31/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4388 - acc: 0.0029 - val_loss: 0.4012 - val_acc: 0.0044\n",
      "Epoch 32/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.4375 - acc: 0.0029 - val_loss: 0.4000 - val_acc: 0.0044\n",
      "Epoch 33/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.4362 - acc: 0.0029 - val_loss: 0.3987 - val_acc: 0.0044\n",
      "Epoch 34/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4350 - acc: 0.0029 - val_loss: 0.3979 - val_acc: 0.0044\n",
      "Epoch 35/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.4339 - acc: 0.0029 - val_loss: 0.3969 - val_acc: 0.0044\n",
      "Epoch 36/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.4328 - acc: 0.0029 - val_loss: 0.3958 - val_acc: 0.0044\n",
      "Epoch 37/100\n",
      "11610/11610==============================] - 0s 26us/sample - loss: 0.4318 - acc: 0.0029 - val_loss: 0.3950 - val_acc: 0.0044\n",
      "Epoch 38/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4307 - acc: 0.0029 - val_loss: 0.3944 - val_acc: 0.0044\n",
      "Epoch 39/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4297 - acc: 0.0029 - val_loss: 0.3934 - val_acc: 0.0044\n",
      "Epoch 40/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4288 - acc: 0.0029 - val_loss: 0.3927 - val_acc: 0.0044\n",
      "Epoch 41/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.4279 - acc: 0.0029 - val_loss: 0.3920 - val_acc: 0.0044\n",
      "Epoch 42/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4270 - acc: 0.0029 - val_loss: 0.3914 - val_acc: 0.0044\n",
      "Epoch 43/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.4262 - acc: 0.0029 - val_loss: 0.3908 - val_acc: 0.0044\n",
      "Epoch 44/100\n",
      "11610/11610==============================] - 0s 35us/sample - loss: 0.4253 - acc: 0.0029 - val_loss: 0.3901 - val_acc: 0.0044\n",
      "Epoch 45/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4245 - acc: 0.0029 - val_loss: 0.3897 - val_acc: 0.0044\n",
      "Epoch 46/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4236 - acc: 0.0029 - val_loss: 0.3892 - val_acc: 0.0044\n",
      "Epoch 47/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4227 - acc: 0.0029 - val_loss: 0.3888 - val_acc: 0.0044\n",
      "Epoch 48/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.4221 - acc: 0.0029 - val_loss: 0.3882 - val_acc: 0.0044\n",
      "Epoch 49/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.4214 - acc: 0.0029 - val_loss: 0.3879 - val_acc: 0.0044\n",
      "Epoch 50/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.4206 - acc: 0.0029 - val_loss: 0.3878 - val_acc: 0.0044\n",
      "Epoch 51/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4198 - acc: 0.0029 - val_loss: 0.3872 - val_acc: 0.0044\n",
      "Epoch 52/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.4189 - acc: 0.0029 - val_loss: 0.3870 - val_acc: 0.0044\n",
      "Epoch 53/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.4183 - acc: 0.0029 - val_loss: 0.3865 - val_acc: 0.0044\n",
      "Epoch 54/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.4174 - acc: 0.0029 - val_loss: 0.3868 - val_acc: 0.0044\n",
      "Epoch 55/100\n",
      "11610/11610==============================] - 0s 38us/sample - loss: 0.4167 - acc: 0.0029 - val_loss: 0.3863 - val_acc: 0.0044\n",
      "Epoch 56/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.4158 - acc: 0.0029 - val_loss: 0.3862 - val_acc: 0.0044\n",
      "Epoch 57/100\n",
      "11610/11610==============================] - 0s 39us/sample - loss: 0.4154 - acc: 0.0029 - val_loss: 0.3857 - val_acc: 0.0044\n",
      "Epoch 58/100\n",
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4145 - acc: 0.0029 - val_loss: 0.3854 - val_acc: 0.0044\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11610/11610==============================] - 0s 37us/sample - loss: 0.4137 - acc: 0.0029 - val_loss: 0.3857 - val_acc: 0.0044\n",
      "Epoch 60/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4130 - acc: 0.0029 - val_loss: 0.3853 - val_acc: 0.0044\n",
      "Epoch 61/100\n",
      "11610/11610==============================] - 0s 41us/sample - loss: 0.4123 - acc: 0.0029 - val_loss: 0.3850 - val_acc: 0.0044\n",
      "Epoch 62/100\n",
      "11610/11610==============================] - 0s 42us/sample - loss: 0.4115 - acc: 0.0029 - val_loss: 0.3853 - val_acc: 0.0044\n",
      "Epoch 63/100\n",
      "11610/11610==============================] - 0s 42us/sample - loss: 0.4107 - acc: 0.0029 - val_loss: 0.3852 - val_acc: 0.0044\n",
      "Epoch 64/100\n",
      "11610/11610==============================] - 0s 42us/sample - loss: 0.4101 - acc: 0.0029 - val_loss: 0.3853 - val_acc: 0.0044\n",
      "Epoch 65/100\n",
      "11610/11610==============================] - 0s 36us/sample - loss: 0.4093 - acc: 0.0029 - val_loss: 0.3853 - val_acc: 0.0044\n",
      "Epoch 66/100\n",
      "11610/11610==============================] - 0s 40us/sample - loss: 0.4086 - acc: 0.0029 - val_loss: 0.3856 - val_acc: 0.0044\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=my_mse, optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid),\n",
    "                   callbacks=[\n",
    "                       keras.callbacks.EarlyStopping(patience=5)\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160==============================] - 0s 13us/sample - loss: 0.4028 - acc: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40279921944751296, 0.002131783]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curves(history)\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 – Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Some layers have no weights, such as `keras.layers.Flatten` or `keras.layers.ReLU`. If you want to create a custom layer without any weights, the simplest option is to create a `keras.layers.Lambda` layer and pass it the function to perform. For example, try creating a custom layer that applies the softplus function (log(exp(X) + 1), and try calling this layer like a regular function. **Tip**: you can use `tf.math.softplus()` rather than computing the log and the exponential manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_softplus = keras.layers.Lambda(lambda X: tf.nn.softplus(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_softplus([-10., -5., 0., 5., 10.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Create a regression model like in exercise 1, but add your softplus layer at the top (i.e., after the existing 1-unit dense layer). This can be useful to ensure that your model never predicts negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "    my_softplus\n",
    "])\n",
    "model.compile(loss=my_portable_mse, optimizer=\"sgd\")\n",
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Alternatively, try using this softplus layer as the activation function of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "#    keras.layers.Dense(1, activation=my_softplus) # Currently broken, see https://github.com/tensorflow/tensorflow/issues/25096\n",
    "    keras.layers.Dense(1, activation=tf.function(lambda X: my_softplus(X)))\n",
    "])\n",
    "\n",
    "# Alternatives...\n",
    "\n",
    "#model = keras.models.Sequential([\n",
    "#    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "#    keras.layers.Dense(1, activation=\"softplus\")\n",
    "#])\n",
    "\n",
    "#model = keras.models.Sequential([\n",
    "#    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "#    keras.layers.Dense(1, activation=keras.activations.softplus)\n",
    "#])\n",
    "\n",
    "#model = keras.models.Sequential([\n",
    "#    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "#    keras.layers.Dense(1),\n",
    "#    keras.layers.Activation(\"softplus\")\n",
    "#])\n",
    "\n",
    "model.compile(loss=my_portable_mse, optimizer=\"sgd\")\n",
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Now let's create a custom layer with its own weights. Use the following template to create a `MyDense` layer that computes $\\phi(\\mathbf{X} \\mathbf{W}) + \\mathbf{b}$, where $\\phi$ is the (optional) activation function, $\\mathbf{X}$ is the input data, $\\mathbf{W}$ represents the kernel (i.e., connection weights), and $\\mathbf{b}$ represents the biases, then train and evaluate a model using this instead of a regular `Dense` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.biases = self.add_weight(name='bias', \n",
    "                                      shape=(self.units,),\n",
    "                                      initializer='zeros',\n",
    "                                      trainable=True)\n",
    "        super(MyDense, self).build(input_shape)\n",
    "\n",
    "    @tf.function   # required, see https://github.com/tensorflow/tensorflow/issues/25096\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    MyDense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 – TensorFlow Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Examine and run the following code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_elu(z, scale=1.0, alpha=1.0):\n",
    "    is_positive = tf.greater_equal(z, 0.0)\n",
    "    return scale * tf.where(is_positive, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1310933, shape=(), dtype=float32, numpy=-0.95021296>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_elu(tf.constant(-3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1310943, shape=(2,), dtype=float32, numpy=array([-0.95021296,  2.5       ], dtype=float32)>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_elu(tf.constant([-3., 2.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.def_function.Function at 0x139dfb358>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_elu_tf = tf.function(scaled_elu)\n",
    "scaled_elu_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1310959, shape=(), dtype=float32, numpy=-0.95021296>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_elu_tf(tf.constant(-3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1310974, shape=(2,), dtype=float32, numpy=array([-0.95021296,  2.5       ], dtype=float32)>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_elu_tf(tf.constant([-3., 2.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_elu_tf.python_function is scaled_elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.55 ms ± 506 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit scaled_elu(tf.random.normal((1000, 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.95 ms ± 921 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit scaled_elu_tf(tf.random.normal((1000, 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tf_code(func):\n",
    "    from IPython.display import display, Markdown\n",
    "    code = tf.autograph.to_code(func)\n",
    "    display(Markdown('```python\\n{}\\n```'.format(code)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from __future__ import print_function\n",
       "\n",
       "def tf__scaled_elu(z, scale=None, alpha=None):\n",
       "  try:\n",
       "    with ag__.function_scope('scaled_elu'):\n",
       "      is_positive = tf.greater_equal(z, 0.0)\n",
       "      return scale * tf.where(is_positive, z, alpha * tf.nn.elu(z))\n",
       "  except:\n",
       "    ag__.rewrite_graph_construction_error(ag_source_map__)\n",
       "\n",
       "\n",
       "\n",
       "tf__scaled_elu.autograph_info__ = {}\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tf_code(scaled_elu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = tf.Variable(0)\n",
    "\n",
    "@tf.function\n",
    "def add_21():\n",
    "    return var.assign_add(21)\n",
    "\n",
    "@tf.function\n",
    "def times_2():\n",
    "    return var.assign(var * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1328045, shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_21()\n",
    "times_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def times_4(x):\n",
    "    return 4. * x\n",
    "\n",
    "@tf.function\n",
    "def times_4_plus_22(x):\n",
    "    return times_4(x) + 22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1328057, shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_4_plus_22(tf.constant(5.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute 1 + 1/2 + 1/4 + ...: the order of execution of the operations with side-effects (e.g., `assign()`) is preserved (in TF 1.x, `tf.control_dependencies()` was needed in such cases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1328143, shape=(), dtype=float32, numpy=1.9999981>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = tf.Variable(0.)\n",
    "increment = tf.Variable(1.)\n",
    "\n",
    "@tf.function\n",
    "def converge_to_2(n_iterations):\n",
    "    for i in tf.range(n_iterations):\n",
    "        total.assign_add(increment)\n",
    "        increment.assign(increment / 2.0)\n",
    "    return total\n",
    "\n",
    "converge_to_2(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Write a function that computes the sum of squares from 1 to n, where n is an argument. Convert it to a graph function by using `tf.function` as a decorator. Display the code generated by autograph using the `display_tf_code()` function. Use `%timeit` to see how must faster the TensorFlow `Function` is compared to the Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sum_of_squares(n):\n",
    "    return tf.math.reduce_sum(tf.square(tf.range(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from __future__ import print_function\n",
       "import tensorflow as tf\n",
       "\n",
       "@tf.function\n",
       "def tf__sum_of_squares(n):\n",
       "  try:\n",
       "    with ag__.function_scope('sum_of_squares'):\n",
       "      return tf.math.reduce_sum(tf.square(tf.range(n)))\n",
       "  except:\n",
       "    ag__.rewrite_graph_construction_error(ag_source_map__)\n",
       "\n",
       "\n",
       "\n",
       "tf__sum_of_squares.autograph_info__ = {}\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tf_code(sum_of_squares.python_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.7 µs ± 1.79 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sum_of_squares(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.5 µs ± 849 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sum_of_squares.python_function(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Examine and run the following code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def square(x):\n",
    "    print(\"Calling\", x)  # part of the TF Function\n",
    "    tf.get_logger().warning(\"Tracing\")  # NOT part of the TF Function\n",
    "    return tf.square(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0202 23:53:04.074125 140735902745472 tmpfmskmmry.py:19] Tracing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling 0\n",
      "Calling 1\n",
      "Calling 2\n",
      "Calling 3\n",
      "Calling 4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    square(tf.constant(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0202 23:53:04.236757 140735902745472 tmp6t8zf9b0.py:19] Tracing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling 0.0\n",
      "Calling 1.0\n",
      "Calling 2.0\n",
      "Calling 3.0\n",
      "Calling 4.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    square(tf.constant(i, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0202 23:53:04.389349 140735902745472 tmpz5okuwrq.py:19] Tracing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling [0. 0.]\n",
      "Calling [1. 1.]\n",
      "Calling [2. 2.]\n",
      "Calling [3. 3.]\n",
      "Calling [4. 4.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    square(tf.constant([i, i], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0202 23:53:04.549182 140735902745472 tmpem9n2y58.py:19] Tracing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0202 23:53:04.686167 140735902745472 tmppjkn_bc4.py:19] Tracing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0202 23:53:04.823907 140735902745472 tmp15zdhqts.py:19] Tracing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0202 23:53:04.965104 140735902745472 tmpl5ove78d.py:19] Tracing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0202 23:53:05.108469 140735902745472 tmpcyy45tad.py:19] Tracing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling 4\n"
     ]
    }
   ],
   "source": [
    "# WARNING: when passing non-tensor values, a trace happens for any new value!\n",
    "# This is to allow optimization in case this value determines e.g., number of layers.\n",
    "for i in range(5):\n",
    "    square(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) When you give Keras a custom loss function, it actually creates a graph function based on it, and then uses that graph function during training. The same is true of custom metric functions, and the `call()` method of custom layers and models. Create a `my_mse()` function, like you did earlier, but add an instruction to log a message inside it (do *not* use `print()`!), and verify that the message is only logged once when you compile and train the model. Optionally, you can also find out when Keras converts custom metrics, layers and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse(y_true, y_pred):\n",
    "    tf.get_logger().warning(\"Tracing metric my_mae()\")\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Examine the following function, and try to call it with various argument types and shapes. Notice that only tensors of type `int32` and one dimension (of any size) are accepted now that we have specified the `input_signature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.int32, name=\"x\")])\n",
    "def cube(z):\n",
    "    return tf.pow(z, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2707226, shape=(3,), dtype=int32, numpy=array([ 1,  8, 27], dtype=int32)>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(tf.constant([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2707229, shape=(5,), dtype=int32, numpy=array([  1,   8,  27,  64, 125], dtype=int32)>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(tf.constant([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TensorSpec(shape=(None,), dtype=tf.int32, name='x')]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube.input_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Function.get_concrete_function of <tensorflow.python.eager.def_function.Function object at 0x141d8da90>>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube.get_concrete_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 – Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Examine the code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Write a function that computes the sum of squares from 1 to n, where n is an argument. Convert it to a graph function by using `tf.function` as a decorator. Display the code generated by autograph using the `display_tf_code()` function. Use `%timeit` to see how must faster the TensorFlow `Function` is compared to the Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sum_squares(n):\n",
    "    s = tf.constant(0)\n",
    "    for i in range(1, n + 1):\n",
    "        s = s + i ** 2\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_squares(tf.constant(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tf_code(sum_squares.python_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sum_squares(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sum_squares.python_function(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Examine the code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) When you give Keras a custom loss function, it actually creates a graph function based on it, and then uses that graph function during training. The same is true of custom metric functions, and the `call()` method of custom layers and models. Create a `my_mse()` function, like you did earlier, but add an instruction to log a message inside it (do *not* use `print()`!), and verify that the message is only logged once when you compile and train the model. Optionally, you can also find out when Keras converts custom metrics, layers and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "def my_mse(y_true, y_pred):\n",
    "    tf.get_logger().warning(\"Tracing loss my_mse()\")\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metric function\n",
    "def my_mae(y_true, y_pred):\n",
    "    tf.get_logger().warning(\"Tracing metric my_mae()\")\n",
    "    return tf.reduce_mean(tf.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.biases = self.add_weight(name='bias', \n",
    "                                      shape=(self.units,),\n",
    "                                      initializer='zeros',\n",
    "                                      trainable=True)\n",
    "        super(MyDense, self).build(input_shape)\n",
    "\n",
    "    def call(self, X):\n",
    "        tf.get_logger().warning(\"Tracing MyDense.call()\")\n",
    "        return self.activation(X @ self.kernel + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model\n",
    "class MyModel(keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.hidden1 = MyDense(30, activation=\"relu\")\n",
    "        self.hidden2 = MyDense(30, activation=\"relu\")\n",
    "        self.output_ = MyDense(1)\n",
    "\n",
    "    def call(self, input):\n",
    "        tf.get_logger().warning(\"Tracing MyModel.call()\")\n",
    "        hidden1 = self.hidden1(input)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input, hidden2])\n",
    "        output = self.output_(concat)\n",
    "        return output\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_mse, optimizer=\"sgd\", metrics=[my_mae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each custom function is traced just once, except for the metric function. That's a bit odd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Examine the following function, and try to call it with various argument types and shapes. Notice that only tensors of type `int32` and one dimension (of any size) are accepted now that we have specified the `input_signature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.int32, name=\"x\")])\n",
    "def cube(z):\n",
    "    return tf.pow(z, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube(tf.constant([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube(tf.constant([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cube([1, 2, 3])\n",
    "except ValueError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cube(tf.constant([1., 2., 3]))\n",
    "except ValueError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    cube(tf.constant([[1, 2], [3, 4]]))\n",
    "except ValueError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 – Function Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Examine and run the following code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.int32, name=\"x\")])\n",
    "def cube(z):\n",
    "    return tf.pow(z, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_func_int32 = cube.get_concrete_function(tf.TensorSpec([None], tf.int32))\n",
    "cube_func_int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_func_int32 is cube.get_concrete_function(tf.TensorSpec([5], tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_func_int32 is cube.get_concrete_function(tf.constant([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_func_int32.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The function's graph is represented on the following diagram. Call the graph's `get_operations()` method to get the list of operations. Each operation has an `inputs` attribute that returns an iterator over its input tensors (these are symbolic: contrary to tensors we have used up to now, they have no value). It also has an `outputs` attribute that returns the list of output tensors. Each tensor has an `op` attribute that returns the operation it comes from. Try navigating through the graph using these methods and attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cube_graph.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Each operation has a default name, such as `\"pow\"` (you can override it by setting the `name` attribute when you call the operation). In case of a name conflict, TensorFlow adds an underscore and anindex to make the name unique (e.g. `\"pow_1\"`). Moreover, each tensor has the same name as the operation that outputs it, followed by a colon `:` and the tensor's `index` (e.g., `\"pow:0\"`). Most operations have a single output tensor, so most tensors have a name that ends with `:0`. Try using `get_operation_by_name()` and `get_tensor_by_name()` to access any op and tensor you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Call the graph's `as_graph_def()` method and print the output. This is a protobuf representation of the computation graph: it is what makes TensorFlow models so portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Get the concrete function's `function_def`, and look at its `signature`. This shows the names and types of the nodes in the graph that correspond to the function's inputs and outputs. This will come in handy when you deploy models to TensorFlow Serving or Google Cloud ML Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 – Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Examine the code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The function's graph is represented on the following diagram. Call the graph's `get_operations()` method to get the list of operations. Each operation has an `inputs` attribute that returns an iterator over its input tensors (these are symbolic: contrary to tensors we have used up to now, they have no value). It also has an `outputs` attribute that returns the list of output tensors. Each tensor has an `op` attribute that returns the operation it comes from. Try navigating through the graph using these methods and attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cube_graph.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_func_int32.graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow_op = cube_func_int32.graph.get_operations()[2]\n",
    "pow_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow_in = list(pow_op.inputs)\n",
    "pow_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow_out = list(pow_op.outputs)\n",
    "pow_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow_in = list(pow_op.inputs)\n",
    "pow_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow_in[0].op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Each operation has a default name, such as `\"pow\"` (you can override it by setting the `name` attribute when you call the operation). In case of a name conflict, TensorFlow adds an underscore and anindex to make the name unique (e.g. `\"pow_1\"`). Moreover, each tensor has the same name as the operation that outputs it, followed by a colon `:` and the tensor's `index` (e.g., `\"pow:0\"`). Most operations have a single output tensor, so most tensors have a name that ends with `:0`. Try using `get_operation_by_name()` and `get_tensor_by_name()` to access any op and tensor you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_func_int32.graph.get_operation_by_name(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_func_int32.graph.get_tensor_by_name(\"x:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Call the graph's `as_graph_def()` method and print the output. This is a protobuf representation of the computation graph: it is what makes TensorFlow models so portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_func_int32.graph.as_graph_def()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Get the concrete function's `function_def`, and look at its `signature`. This shows the names and types of the nodes in the graph that correspond to the function's inputs and outputs. This will come in handy when you deploy models to TensorFlow Serving or Google Cloud ML Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cube_func_int32.function_def.signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 – Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Examine and run the following code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3. * x ** 2 + 2. * x - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_derivative(f, x, eps=1e-3):\n",
    "    return (f(x + eps) - f(x - eps)) / (2. * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximate_derivative(f, 1.0) # true derivative = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-2, 2, 200)\n",
    "fs = f(xs)\n",
    "x0 = 0.5\n",
    "df_x0 = approximate_derivative(f, x0)\n",
    "tangent_x0 = df_x0 * (xs - x0) + f(x0)\n",
    "plt.plot([-2, 2], [0, 0], \"k-\", linewidth=1)\n",
    "plt.plot([0, 0], [-5, 15], \"k-\", linewidth=1)\n",
    "plt.plot(xs, fs)\n",
    "plt.plot(xs, tangent_x0, \"r--\")\n",
    "plt.plot(x0, f(x0), \"ro\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"f(x)\", fontsize=14, rotation=0)\n",
    "plt.axis([-2, 2, -5, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x1, x2):\n",
    "    return (x1 + 5) * (x2 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_gradient(f, x1, x2, eps=1e-3):\n",
    "    df_x1 = approximate_derivative(lambda x: f(x, x2), x1, eps)\n",
    "    df_x2 = approximate_derivative(lambda x: f(x1, x), x2, eps)\n",
    "    return df_x1, df_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximate_gradient(g, 2.0, 3.0) # true gradient = (9, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(x1, x2)\n",
    "grads = tape.gradient(z, [x1, x2])\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(x1, x2)\n",
    "\n",
    "dz_x1 = tape.gradient(z, x1)\n",
    "try:\n",
    "    dz_x2 = tape.gradient(z, x2)\n",
    "except RuntimeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = g(x1, x2)\n",
    "\n",
    "dz_x1 = tape.gradient(z, x1)\n",
    "dz_x2 = tape.gradient(z, x2)\n",
    "del tape\n",
    "dz_x1, dz_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.constant(2.0) # <= not Variable\n",
    "x2 = tf.constant(3.0) # <= not Variable\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(x1, x2)\n",
    "\n",
    "grads = tape.gradient(z, [x1, x2])\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.constant(2.0)\n",
    "x2 = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x1)\n",
    "    tape.watch(x2)\n",
    "    z = g(x1, x2)\n",
    "\n",
    "grads = tape.gradient(z, [x1, x2])\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = 3 * x\n",
    "    z2 = x ** 2\n",
    "tape.gradient([z1, z2], x) # dz1_x + dz2_x = 3 + 2x = 3 + 2*5 = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape(persistent=True) as hessian_tape:\n",
    "    with tf.GradientTape() as jacobian_tape:\n",
    "        z = g(x1, x2)\n",
    "    jacobians = jacobian_tape.gradient(z, [x1, x2])\n",
    "hessians = [hessian_tape.gradient(jacobian, [x1, x2])\n",
    "            for jacobian in jacobians]\n",
    "del hessian_tape\n",
    "hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Implement Gradient Descent manually to find the value of `x` that minimizes the following function `f(x)`.\n",
    "\n",
    "**Tips**:\n",
    "* Define a variable `x` and initialize it to 0.\n",
    "* Define the `learning_rate` (e.g., 0.1).\n",
    "* Write a loop that will repeatedly (1) compute the gradient of `f` (actually a derivative in this case) at the current value of `x`, and (2) tweak `x` slightly in the opposite direction (by subtracting `learning_rate * df_dx`). You can use `x.assign_sub(...)` for this.\n",
    "* Using calculus, we can find that the algorithm should converge to $x = -\\frac{1}{3}$. Indeed, the derivative of $f(x) = 3 x^2 + 2x -1$ is $f'(x) = 6x + 2$, so the minimum is reached when $f'(x) = 0$ (slope is 0), so $6x + 2 = 0$, which leads to $x = -\\frac{1}{3}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3. * x ** 2 + 2. * x - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Now use an `SGD` optimizer instead of manually tweaking `x`.\n",
    "\n",
    "**Tips**:\n",
    "* You first need to create an `SGD` optimizer, optionally specifying the learning_rate (e.g., `lr=0.1`).\n",
    "* Next replace the manual tweaking of `x` in your previous code to use `optimizer.apply_gradients()` instead. You need to pass it a list of gradient/variable pairs (just one pair in this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Create a `Sequential` model for the California housing problem (no need to compile it), and train it using your own training loop, instead of using `fit()`. Evaluate your model on the validation set at the end of each epoch, and display the result.\n",
    "\n",
    "**Tips**:\n",
    "* You can use the following `random_batch()` function to get a new batch of training data at each iteration (the Data API would be much preferable, as we will see in the next notebook).\n",
    "* You can use the model like a function to make predictions: `y_pred = model(X_batch)`\n",
    "* You can use `keras.losses.mean_squared_error()` to compute the loss. Note that it returns one loss per instance, so you need to use `tf.reduce_mean()` to get the mean loss. \n",
    "* You can use `model.trainable_variables` to get the full list of trainable variables in your model.\n",
    "* You can use `zip(gradients, variables)` to create a list containing all the gradient/variable pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size = 32):\n",
    "    idx = np.random.randint(0, len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Examine and run the following code examples, then update your training loop to display the training loss at each iteration.\n",
    "\n",
    "**Tips**:\n",
    "* You can use a `keras.metrics.MeanSquaredError` instance to efficiently track the running mean squared error at each iteration.\n",
    "* Make sure you reset the metric's states at the start of each epoch.\n",
    "* You can use `print(\"\\r\", mse, end=\"\")` to display the MSE on the same line at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = keras.metrics.MeanSquaredError()\n",
    "metric([5.], [2.])  # error = (2 - 5)**2 = 9\n",
    "metric([0.], [1.])  # error = (1 - 0)**2 = 1\n",
    "metric.result()     # mean error = (9 + 1) / 2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.reset_states()\n",
    "metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric([1.], [3.])  # error = (3 - 1)**2 = 4\n",
    "metric.result()     # mean error = 4 / 1 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 – Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Examine the code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Implement Gradient Descent manually to find the value of `x` that minimizes the following function `f(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3. * x ** 2 + 2. * x - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "x = tf.Variable(0.0)\n",
    "\n",
    "for iteration in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    dz_dx = tape.gradient(z, x)\n",
    "    x.assign_sub(learning_rate * dz_dx)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Now use an `SGD` optimizer instead of manually tweaking `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(0.0)\n",
    "optimizer = keras.optimizers.SGD(lr=0.1)\n",
    "\n",
    "for iteration in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    dz_dx = tape.gradient(z, x)\n",
    "    optimizer.apply_gradients([(dz_dx, x)])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Create a `Sequential` model for the California housing problem (no need to compile it), and train it using your own training loop, instead of using `fit()`. Evaluate your model on the validation set at the end of each epoch, and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size = 32):\n",
    "    idx = np.random.randint(0, len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.SGD()\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step in range(steps_per_epoch):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train, batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        grads_and_vars = zip(grads, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "    y_pred = model(X_valid_scaled)\n",
    "    valid_loss = tf.reduce_mean(loss_fn(y_valid, y_pred))\n",
    "    print(\"Epoch\", epoch, \"valid mse:\", valid_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Examine and run the following code examples, then update your training loop to display the training loss at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = keras.metrics.MeanSquaredError()\n",
    "metric([5.], [2.])  # error = (2 - 5)**2 = 9\n",
    "metric([0.], [1.])  # error = (1 - 0)**2 = 1\n",
    "metric.result()     # mean error = (9 + 1) / 2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.reset_states()\n",
    "metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric([1.], [3.])  # error = (3 - 1)**2 = 4\n",
    "metric.result()     # mean error = 4 / 1 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.SGD()\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "metric = keras.metrics.MeanSquaredError()  # ADDED\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    metric.reset_states()  # ADDED\n",
    "    for step in range(steps_per_epoch):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train, batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            metric(y_batch, y_pred)  # ADDED\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        grads_and_vars = zip(grads, model.trainable_variables)\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "        print(\"\\rEpoch\", epoch, \" train mse:\", metric.result().numpy(), end=\"\")  # ADDED\n",
    "    y_pred = model(X_valid_scaled)\n",
    "    valid_loss = tf.reduce_mean(loss_fn(y_valid, y_pred))\n",
    "    print(\"\\tvalid mse:\", valid_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You now know how to use TensorFlow's low-level API to write custom loss functions, layers, and models. You also learned how to optimize your functions by converting them to graphs: this allows TensorFlow to run operations in parallel and to perform various optimizations. Next, you learned how TensorFlow Functions and graphs are structured, and how to navigate through them. Finally, you learned how to use autodiff and write your own custom training loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
